多任务学习是相对单任务学习的概念，考虑到一些任务的训练数据稀缺或者bias问题等，同时将两个及以上的任务一起进行训练，比如在推荐系统中，排序模型同时预估候选的点击率和停留时长，多任务学习有以下优势：\
* 多个任务共享一个模型，占用内存减少（训练开销）\
* 多个任务一次前向计算得出结果，推理速度加快（serving算力开销）\
* 关联任务通过共享信息，相互补充，可以提升彼此的表现（知识迁移）\
但是多任务也有很多挑战，经常出现跷跷板现象，一个任务效果变好，另一个任务效果变差，究其原因，核心还是训练过程中存在以下3个方面问题\
* 多任务梯度方向不一致：同一组参数，不同任务的更新方向不一致，导致模型参数出现震荡，任务之间出现负迁移现象，一般出现在多个任务之间差异较大的场景\
* 多任务收敛速度不一致：不同任务收敛速度不一样，有的任务收敛速度快，有的任务收敛速度慢，导致模型训练多轮后，有一些overfitting，有一些underfitting\
* 多任务loss取值量级差异大：不同的任务loss取值范围差异大，模型被loss比较大的任务主导，这种情况再两个任务使用不同损失函数，或者拟合值取值差异大等情况下\

**Share Bottom**\
最直观的多任务实现方式，通过共享底层编码器，每个任务都拥有独立的尾部前馈神经网络和预测层，该架构有以下特点：\
1. 共享特征表示
2. 独立任务处理
3. 简化模型结构
4. 解耦任务关系\
但是该结构也有比较明显的缺点：任务之间信息的交流比较少\
**MMoE**\
mmoe系列主要改进了共享部分，将共享编码器抽象成多个专家模块，每个专家模块能够学到不同领域的知识，在多个专家之后，引入了一个门控网络，该网络负责控制每个专家对于每个任务的贡献程度，MMoE在MoE的基础上，进一步拆解，每个任务都拥有自己的门控机制
**参考文献**  
1、https://cloud.tencent.com/developer/article/2415348  
2、https://oysterqaq.com/archives/1521  
3、https://www.geeksforgeeks.org/deep-learning/introduction-to-multi-task-learningmtl-for-deep-learning/  
4、https://medium.com/gumgum-tech/multi-task-learning-what-is-it-how-does-it-work-and-why-does-it-work-294769c457bb  
5、https://arxiv.org/pdf/2404.18961  
6、https://zhuanlan.zhihu.com/p/456089764  
7、https://zhuanlan.zhihu.com/p/425209494  
8、https://zhuanlan.zhihu.com/p/348873723  
9、https://www.cnblogs.com/happyNLP/p/16880714.html  
10、https://cloud.tencent.com/developer/article/2117004  
11、https://blog.csdn.net/xq151750111/article/details/127228917  
12、https://spaces.ac.cn/archives/8870  
13、https://spaces.ac.cn/archives/8896  
14、https://zhuanlan.zhihu.com/p/59413549  
